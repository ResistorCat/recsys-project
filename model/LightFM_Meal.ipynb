{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ResistorCat/recsys-project/blob/feat%2Fnew_model/model/LightFM_Meal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJuMSsEwEAqG"
      },
      "outputs": [],
      "source": [
        "FLAG_DATASET_DL = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Carga de datos y librerías"
      ],
      "metadata": {
        "id": "bS3zTZV7ENJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actualizar pip\n",
        "%pip install --upgrade pip\n",
        "# Dependencias para graficar y manipular datos\n",
        "%pip install pandas matplotlib tqdm seaborn ipywidgets\n",
        "# Dependencias para leer archivos Parquet\n",
        "%pip install pyarrow fastparquet\n",
        "# Para predicción\n",
        "%pip install scikit-learn lightfm recommenders"
      ],
      "metadata": {
        "id": "2StcEdliEOhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías globales\n",
        "\n",
        "# Manejo de datos y visualización\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "import zipfile\n",
        "\n",
        "\n",
        "# Crear directorios\n",
        "PATH_DATASETS = \"datasets\"\n",
        "PATH_DATASETS_MEALRECPLUS = os.path.join(PATH_DATASETS, \"mealrecplus\")\n",
        "os.makedirs(PATH_DATASETS, exist_ok=True)"
      ],
      "metadata": {
        "id": "hivcX7koEPAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_MEALRECPLUS_META_DATA = \"/content/datasets/mealrecplus/MealRecPlus-main/MealRec+/MealRec+H/meta_data/\"\n",
        "PATH_MEALRECPLUS_HEALTHINESS = \"/content/datasets/mealrecplus/MealRecPlus-main/MealRec+/MealRec+H/healthiness/\""
      ],
      "metadata": {
        "id": "zoN9-yQUEjiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if FLAG_DATASET_DL:\n",
        "  # Descargar \"MealRecPlus\" de WUT-IDEA\n",
        "  os.system(f\"curl -L -o {PATH_DATASETS_MEALRECPLUS}.zip https://github.com/WUT-IDEA/MealRecPlus/archive/refs/heads/main.zip\")\n",
        "  os.system(f\"unzip -o {PATH_DATASETS_MEALRECPLUS}.zip -d {PATH_DATASETS_MEALRECPLUS}\")\n",
        "  # Eliminar el zip\n",
        "  os.remove(f\"{PATH_DATASETS_MEALRECPLUS}.zip\")\n",
        "  # Cita: Ming Li, Lin Li, Xiaohui Tao, and Jimmy Xiangji Huang. 2024. MealRec+: A Meal Recommendation Dataset with Meal-Course Affiliation for Personal- ization and Healthiness. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’24), July 14–18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3626772.3657857 (https://github.com/WUT-IDEA/MealRecPlus)\n",
        "  zip_path_course = PATH_MEALRECPLUS_META_DATA+\"course.zip\"\n",
        "  zip_path_user_course = PATH_MEALRECPLUS_META_DATA+\"user_course.zip\"\n",
        "  extract_path_course = PATH_MEALRECPLUS_META_DATA\n",
        "\n",
        "  os.makedirs(extract_path_course, exist_ok=True)\n",
        "\n",
        "  with zipfile.ZipFile(zip_path_course, 'r') as zip_ref:\n",
        "      zip_ref.extractall(extract_path_course)\n",
        "\n",
        "  with zipfile.ZipFile(zip_path_user_course, 'r') as zip_ref:\n",
        "      zip_ref.extractall(extract_path_course)\n",
        "\n",
        "  print(\"✅ Archivos descomprimidos en:\", extract_path_course)\n"
      ],
      "metadata": {
        "id": "hLo3TYy2Emv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocesamiento de datos"
      ],
      "metadata": {
        "id": "-0gmvKqqE4Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar interacciones usuario-plato (ratings)\n",
        "df_user_course = pd.read_csv(\n",
        "    PATH_MEALRECPLUS_META_DATA+\"user_course.csv\",\n",
        "    names=[\"user_id\", \"course_id\", \"rating\", \"dateLastModified\"],\n",
        "    header=None\n",
        ")\n",
        "\n",
        "# Convertir la columna 'rating' a tipo numérico, forzando los errores a NaN\n",
        "df_user_course['rating'] = pd.to_numeric(df_user_course['rating'], errors='coerce')\n",
        "df_user_course.dropna(subset=['rating'], inplace=True)\n",
        "\n",
        "df_course = pd.read_csv(PATH_MEALRECPLUS_META_DATA+\"course.csv\")\n",
        "\n",
        "df_user2index = pd.read_csv(PATH_MEALRECPLUS_META_DATA+\"user2index.txt\", sep=\"\\t\", names=[\"user_id\", \"user_index\"])\n",
        "df_course2index = pd.read_csv(PATH_MEALRECPLUS_META_DATA+\"course2index.txt\", sep=\"\\t\", names=[\"course_id\", \"course_index\"])"
      ],
      "metadata": {
        "id": "FwEN-STTE7_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_user_course.head()"
      ],
      "metadata": {
        "id": "XNugtbTvF1k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo Base"
      ],
      "metadata": {
        "id": "2ZxYSP_mGGEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from lightfm.cross_validation import random_train_test_split\n",
        "from lightfm.evaluation import precision_at_k, recall_at_k\n",
        "\n",
        "\n",
        "class MealRecPlusRecommender:\n",
        "    def __init__(self,\n",
        "                 df_ratings: pd.DataFrame,\n",
        "                 min_ratings_per_user: int = 5,\n",
        "                 min_ratings_per_item: int = 5,\n",
        "                 test_percentage: float = 0.25,\n",
        "                 no_components: int = 30,\n",
        "                 learning_rate: float = 0.05,\n",
        "                 loss: str = 'warp',\n",
        "                 random_state: int = 42,\n",
        "                 num_threads: int = 4,\n",
        "                 verbose: bool = False):\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Filtrado\n",
        "        user_counts = df_ratings['user_id'].value_counts()\n",
        "        item_counts = df_ratings['course_id'].value_counts()\n",
        "        keep_users  = user_counts[user_counts >= min_ratings_per_user].index\n",
        "        keep_items  = item_counts[item_counts >= min_ratings_per_item].index\n",
        "        self.df = df_ratings[\n",
        "            df_ratings['user_id'].isin(keep_users) &\n",
        "            df_ratings['course_id'].isin(keep_items)\n",
        "        ].copy()\n",
        "\n",
        "        # Dataset\n",
        "        self.dataset = Dataset()\n",
        "        self.dataset.fit(\n",
        "            users=self.df['user_id'].unique(),\n",
        "            items=self.df['course_id'].unique()\n",
        "        )\n",
        "\n",
        "        # Interactions\n",
        "        interaction_tuples = self.df[['user_id','course_id','rating']].to_numpy()\n",
        "        self.interactions, _ = self.dataset.build_interactions(interaction_tuples)\n",
        "\n",
        "        # Split\n",
        "        self.train, self.test = random_train_test_split(\n",
        "            self.interactions,\n",
        "            test_percentage=test_percentage,\n",
        "            random_state=np.random.RandomState(random_state)\n",
        "        )\n",
        "\n",
        "        # Modelo\n",
        "        self.model = LightFM(\n",
        "            loss=loss,\n",
        "            no_components=no_components,\n",
        "            learning_rate=learning_rate,\n",
        "            random_state=np.random.RandomState(random_state)\n",
        "        )\n",
        "        self.num_threads = num_threads\n",
        "\n",
        "    def fit(self, epochs: int = 10):\n",
        "        \"\"\"Entrena el modelo WARP sin sample_weight.\"\"\"\n",
        "        self.model.fit(\n",
        "            self.train,\n",
        "            epochs=epochs,\n",
        "            num_threads=self.num_threads,\n",
        "            verbose=self.verbose\n",
        "        )\n",
        "\n",
        "    def evaluate(self, k: int = 10) -> dict:\n",
        "        prec = precision_at_k(self.model, self.test,\n",
        "                              train_interactions=self.train,\n",
        "                              k=k, num_threads=self.num_threads).mean()\n",
        "        rec = recall_at_k(self.model, self.test,\n",
        "                          train_interactions=self.train,\n",
        "                          k=k, num_threads=self.num_threads).mean()\n",
        "        return {f'precision@{k}': prec, f'recall@{k}': rec}\n",
        "\n",
        "    def recommend(self, user_id, num_items: int = 10) -> pd.DataFrame:\n",
        "        n_users, n_items = self.dataset.interactions_shape()\n",
        "        scores = self.model.predict(user_id,\n",
        "                                    np.arange(n_items),\n",
        "                                    num_threads=self.num_threads)\n",
        "        _, _, idx_to_item = self.dataset.mapping()\n",
        "        inv_map = {v: k for k, v in idx_to_item.items()}\n",
        "        top_idx = np.argsort(-scores)[:num_items]\n",
        "        return pd.DataFrame({\n",
        "            'course_id': [inv_map[i] for i in top_idx],\n",
        "            'score':     scores[top_idx]\n",
        "        })\n",
        "\n",
        "    def save(self, path: str):\n",
        "        payload = {\n",
        "            'model': self.model,\n",
        "            'dataset': self.dataset,\n",
        "            'train': self.train,\n",
        "            'test': self.test,\n",
        "            'num_threads': self.num_threads,\n",
        "            'verbose': self.verbose,\n",
        "        }\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(payload, f)\n",
        "        if self.verbose:\n",
        "            print(f\"[MealRecPlusRec] Guardado en '{path}'\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str, verbose: bool = False):\n",
        "        with open(path, 'rb') as f:\n",
        "            payload = pickle.load(f)\n",
        "        rec = cls.__new__(cls)\n",
        "        rec.model        = payload['model']\n",
        "        rec.dataset      = payload['dataset']\n",
        "        rec.train        = payload['train']\n",
        "        rec.test         = payload['test']\n",
        "        rec.num_threads  = payload['num_threads']\n",
        "        rec.verbose      = verbose\n",
        "        if verbose:\n",
        "            print(f\"[MealRecPlusRec] Cargado desde '{path}'\")\n",
        "        return rec\n"
      ],
      "metadata": {
        "id": "KhM2XfebGItN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mrp = MealRecPlusRecommender(df_user_course,\n",
        "                             min_ratings_per_user=10,\n",
        "                             min_ratings_per_item=10,\n",
        "                             test_percentage=0.2,\n",
        "                             verbose=True)\n",
        "mrp.fit(epochs=20)\n",
        "print(mrp.evaluate(k=10))\n"
      ],
      "metadata": {
        "id": "o8qYvDnfGM3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo con metadatos basado en salud"
      ],
      "metadata": {
        "id": "eCKPf9vMDykT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from lightfm.cross_validation import random_train_test_split\n",
        "from lightfm.evaluation import precision_at_k, recall_at_k\n",
        "\n",
        "df_meta_course = pd.read_csv(\n",
        "    PATH_MEALRECPLUS_META_DATA+\"course.csv\",\n",
        "    names=[\"course_id\",\"course_name\"]\n",
        ")\n",
        "\n",
        "user_fsa = pd.read_csv(PATH_MEALRECPLUS_HEALTHINESS + \"user_fsa.txt\",\n",
        "                       header=None, names=[\"fsa_score\"])\n",
        "user_who = pd.read_csv(PATH_MEALRECPLUS_HEALTHINESS + \"user_who.txt\",\n",
        "                       header=None, names=[\"who_score\"])\n",
        "course_fsa = pd.read_csv(PATH_MEALRECPLUS_HEALTHINESS + \"course_fsa.txt\",\n",
        "                         header=None, names=[\"fsa_score\"])\n",
        "course_who = pd.read_csv(PATH_MEALRECPLUS_HEALTHINESS + \"course_who.txt\",\n",
        "                         header=None, names=[\"who_score\"])\n",
        "\n",
        "user_fsa[\"user_id\"] = np.arange(len(user_fsa))\n",
        "user_who[\"user_id\"] = np.arange(len(user_who))\n",
        "N = len(course_fsa)\n",
        "df_scores = df_meta_course.iloc[:N][[\"course_id\"]].copy()\n",
        "df_scores[\"fsa_score\"] = course_fsa[\"fsa_score\"].values\n",
        "df_scores[\"who_score\"] = course_who[\"who_score\"].values\n",
        "\n",
        "df_meta_scored = df_meta_course.merge(\n",
        "    df_scores, on=\"course_id\", how=\"inner\"\n",
        ")\n",
        "df_user_scores = pd.merge(user_fsa, user_who, on=\"user_id\")\n",
        "\n",
        "# ————————————————————————————————————————————————————————————————\n",
        "# 2) Bucketización de los promedios de usuario\n",
        "# ————————————————————————————————————————————————————————————————\n",
        "def bucket_fsa(x):\n",
        "    if   x >= 7: return \"u_fsa_high\"\n",
        "    elif x >= 4: return \"u_fsa_mid\"\n",
        "    else:        return \"u_fsa_low\"\n",
        "\n",
        "def bucket_who(x):\n",
        "    if   x >= 7: return \"u_who_high\"\n",
        "    elif x >= 4: return \"u_who_mid\"\n",
        "    else:        return \"u_who_low\"\n",
        "\n",
        "df_user_scores[\"u_fsa_bucket\"] = df_user_scores[\"fsa_score\"].apply(bucket_fsa)\n",
        "df_user_scores[\"u_who_bucket\"] = df_user_scores[\"who_score\"].apply(bucket_who)\n",
        "\n",
        "item_features_map = {\n",
        "    row.course_id: [\n",
        "        f\"fsa:{bucket_fsa(row.fsa_score)}\",\n",
        "        f\"who:{bucket_who(row.who_score)}\"\n",
        "    ]\n",
        "    for row in df_meta_scored.itertuples()\n",
        "}\n",
        "\n",
        "user_feat_tuples = [\n",
        "    (row.user_id, [row.u_fsa_bucket, row.u_who_bucket])\n",
        "    for row in df_user_scores.itertuples()\n",
        "]\n",
        "\n",
        "\n",
        "all_users = df_user_course[\"user_id\"].unique()\n",
        "all_items = df_user_course[\"course_id\"].unique()\n",
        "\n",
        "# Extraigo la lista de user_features únicas\n",
        "all_user_feats = {feat for _, feats in user_feat_tuples for feat in feats}\n",
        "\n",
        "dataset = Dataset()\n",
        "dataset.fit(\n",
        "    users=all_users,\n",
        "    items=all_items,\n",
        "    user_features=list(all_user_feats),\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "interactions, _ = dataset.build_interactions(\n",
        "    df_user_course[[\"user_id\",\"course_id\",\"rating\"]].to_numpy()\n",
        ")\n",
        "train, test = random_train_test_split(interactions,\n",
        "                                      test_percentage=0.25,\n",
        "                                      random_state=42)\n",
        "\n",
        "# FILTRAR user_feat_tuples para que solo queden IDs válidos:\n",
        "valid_users = set(df_user_course[\"user_id\"].unique())\n",
        "user_feat_tuples = [\n",
        "    (uid, feats)\n",
        "    for uid, feats in user_feat_tuples\n",
        "    if uid in valid_users\n",
        "]\n",
        "\n",
        "user_features = dataset.build_user_features(user_feat_tuples)\n",
        "\n",
        "item_feat_tuples = (\n",
        "    (item, item_features_map.get(item, []))\n",
        "    for item in all_items\n",
        ")\n",
        "item_features = dataset.build_item_features(item_feat_tuples)\n",
        "\n",
        "model = LightFM(loss=\"warp\",\n",
        "                no_components=50,\n",
        "                learning_rate=0.02,\n",
        "                item_alpha=1e-4,\n",
        "                user_alpha=1e-4,\n",
        "                random_state=42)\n",
        "\n",
        "model.fit(\n",
        "    train,\n",
        "    user_features=user_features,\n",
        "    item_features=item_features,\n",
        "    epochs=40,\n",
        "    num_threads=4,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "prec = precision_at_k(model, test,\n",
        "                      train_interactions=train,\n",
        "                      user_features=user_features,\n",
        "                      item_features=item_features,\n",
        "                      k=10, num_threads=4).mean()\n",
        "\n",
        "rec = recall_at_k(model, test,\n",
        "                  train_interactions=train,\n",
        "                  user_features=user_features,\n",
        "                  item_features=item_features,\n",
        "                  k=10, num_threads=4).mean()\n",
        "\n",
        "print(f\"precision@10: {prec:.4f}, recall@10: {rec:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkqyeqI6DyTr",
        "outputId": "f0c5dcbe-dfa9-43f7-c538-70f4a7919e08"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 40/40 [00:12<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision@10: 0.1351, recall@10: 0.0731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrNiQm9MnoKK",
        "outputId": "c844c287-40da-4d2c-81d9-98f6d3753978"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->torch_geometric) (3.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mejoras al modelo base\n"
      ],
      "metadata": {
        "id": "mIMd1eQhN8ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from lightfm.cross_validation import random_train_test_split\n",
        "from lightfm.evaluation import precision_at_k, recall_at_k\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Enhanced feature engineering\n",
        "def create_enhanced_buckets(score, score_type='fsa'):\n",
        "    \"\"\"Create more granular buckets for better feature representation\"\"\"\n",
        "    if score_type == 'fsa':\n",
        "        if score >= 13: return f\"{score_type}_very_high\"\n",
        "        elif score >= 11: return f\"{score_type}_high\"\n",
        "        elif score >= 7: return f\"{score_type}_mid_high\"\n",
        "        elif score >= 4: return f\"{score_type}_mid\"\n",
        "        elif score >= 1: return f\"{score_type}_low\"\n",
        "        else: return f\"{score_type}_very_low\"\n",
        "    else:  # WHO\n",
        "        if score >= 4: return f\"{score_type}_high\"\n",
        "        elif score >= 3: return f\"{score_type}_mid_high\"\n",
        "        elif score >= 2: return f\"{score_type}_mid\"\n",
        "        elif score >= 1: return f\"{score_type}_low\"\n",
        "        else: return f\"{score_type}_very_low\"\n",
        "\n",
        "# Apply enhanced bucketing\n",
        "df_user_scores[\"u_fsa_bucket\"] = df_user_scores[\"fsa_score\"].apply(\n",
        "    lambda x: create_enhanced_buckets(x, 'fsa')\n",
        ")\n",
        "df_user_scores[\"u_who_bucket\"] = df_user_scores[\"who_score\"].apply(\n",
        "    lambda x: create_enhanced_buckets(x, 'who')\n",
        ")\n",
        "\n",
        "# Create combined features for better representation\n",
        "df_user_scores[\"u_health_profile\"] = (\n",
        "    df_user_scores[\"u_fsa_bucket\"] + \"_\" + df_user_scores[\"u_who_bucket\"]\n",
        ")\n",
        "\n",
        "# Enhanced item features\n",
        "item_features_map = {}\n",
        "for row in df_meta_scored.itertuples():\n",
        "    fsa_bucket = create_enhanced_buckets(row.fsa_score, 'fsa')\n",
        "    who_bucket = create_enhanced_buckets(row.who_score, 'who')\n",
        "\n",
        "    item_features_map[row.course_id] = [\n",
        "        fsa_bucket,\n",
        "        who_bucket,\n",
        "        f\"{fsa_bucket}_{who_bucket}\",  # Combined feature\n",
        "        f\"fsa_score_{int(row.fsa_score)}\",  # Exact score as feature\n",
        "        f\"who_score_{int(row.who_score)}\"   # Exact score as feature\n",
        "    ]\n",
        "\n",
        "# Enhanced user features\n",
        "user_feat_tuples = []\n",
        "for row in df_user_scores.itertuples():\n",
        "    features = [\n",
        "        row.u_fsa_bucket,\n",
        "        row.u_who_bucket,\n",
        "        row.u_health_profile,\n",
        "        f\"fsa_score_{int(row.fsa_score)}\",\n",
        "        f\"who_score_{int(row.who_score)}\"\n",
        "    ]\n",
        "    user_feat_tuples.append((row.user_id, features))\n",
        "\n",
        "all_users = df_user_course[\"user_id\"].unique()\n",
        "all_items = df_user_course[\"course_id\"].unique()\n",
        "\n",
        "# Extract all unique features\n",
        "all_user_feats = {feat for _, feats in user_feat_tuples for feat in feats}\n",
        "all_item_feats = {feat for feats in item_features_map.values() for feat in feats}\n",
        "\n",
        "dataset = Dataset()\n",
        "dataset.fit(\n",
        "    users=all_users,\n",
        "    items=all_items,\n",
        "    user_features=list(all_user_feats),\n",
        "    item_features=list(all_item_feats)\n",
        ")\n",
        "\n",
        "interactions, _ = dataset.build_interactions(\n",
        "    df_user_course[[\"user_id\",\"course_id\",\"rating\"]].to_numpy()\n",
        ")\n",
        "train, test = random_train_test_split(interactions,\n",
        "                                      test_percentage=0.25,\n",
        "                                      random_state=42)\n",
        "\n",
        "# Filter valid users\n",
        "valid_users = set(df_user_course[\"user_id\"].unique())\n",
        "user_feat_tuples = [\n",
        "    (uid, feats)\n",
        "    for uid, feats in user_feat_tuples\n",
        "    if uid in valid_users\n",
        "]\n",
        "\n",
        "user_features = dataset.build_user_features(user_feat_tuples)\n",
        "\n",
        "item_feat_tuples = [\n",
        "    (item, item_features_map.get(item, []))\n",
        "    for item in all_items\n",
        "]\n",
        "item_features = dataset.build_item_features(item_feat_tuples)\n",
        "\n",
        "def optimize_hyperparameters(train, test, user_features, item_features):\n",
        "    \"\"\"\n",
        "    Grid search for optimal hyperparameters\n",
        "    \"\"\"\n",
        "    param_grid = {\n",
        "        'no_components': [64, 100, 128],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'item_alpha': [1e-6, 1e-5, 1e-4],\n",
        "        'user_alpha': [1e-6, 1e-5, 1e-4],\n",
        "        'loss': ['warp', 'bpr']\n",
        "    }\n",
        "\n",
        "    best_precision = 0\n",
        "    best_params = None\n",
        "\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        model = LightFM(\n",
        "            loss=params['loss'],\n",
        "            no_components=params['no_components'],\n",
        "            learning_rate=params['learning_rate'],\n",
        "            item_alpha=params['item_alpha'],\n",
        "            user_alpha=params['user_alpha'],\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        model.fit(\n",
        "            train,\n",
        "            user_features=user_features,\n",
        "            item_features=item_features,\n",
        "            epochs=30,\n",
        "            num_threads=4,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        precision = precision_at_k(\n",
        "            model, test,\n",
        "            train_interactions=train,\n",
        "            user_features=user_features,\n",
        "            item_features=item_features,\n",
        "            k=10, num_threads=4\n",
        "        ).mean()\n",
        "\n",
        "        if precision > best_precision:\n",
        "            best_precision = precision\n",
        "            best_params = params\n",
        "\n",
        "        print(f\"Params: {params}, Precision@10: {precision:.4f}\")\n",
        "\n",
        "    return best_params, best_precision\n",
        "\n",
        "# Run hyperparameter optimization\n",
        "best_params, best_precision = optimize_hyperparameters(train, test, user_features, item_features)\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Best precision: {best_precision:.4f}\")\n",
        "\n",
        "# Train final model with best parameters\n",
        "final_model = LightFM(\n",
        "    loss=best_params['loss'],\n",
        "    no_components=best_params['no_components'],\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    item_alpha=best_params['item_alpha'],\n",
        "    user_alpha=best_params['user_alpha'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "final_model.fit(\n",
        "    train,\n",
        "    user_features=user_features,\n",
        "    item_features=item_features,\n",
        "    epochs=50,  # More epochs for final model\n",
        "    num_threads=4,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Evaluate final model\n",
        "final_precision = precision_at_k(\n",
        "    final_model, test,\n",
        "    train_interactions=train,\n",
        "    user_features=user_features,\n",
        "    item_features=item_features,\n",
        "    k=10, num_threads=4\n",
        ").mean()\n",
        "\n",
        "final_recall = recall_at_k(\n",
        "    final_model, test,\n",
        "    train_interactions=train,\n",
        "    user_features=user_features,\n",
        "    item_features=item_features,\n",
        "    k=10, num_threads=4\n",
        ").mean()\n",
        "\n",
        "print(f\"Final precision@10: {final_precision:.4f}, recall@10: {final_recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8Ov6HcHhnf4K",
        "outputId": "0bafb55e-57b8-44d7-c6b1-6178f65c4948"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_params' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-16-1330624077.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;31m# Run hyperparameter optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m#best_params, best_precision = optimize_hyperparameters(train, test, user_features, item_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best parameters: {best_params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best precision: {best_precision:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_params' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mejor hiperparametros obtenidos"
      ],
      "metadata": {
        "id": "IXObY0z2OGy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = LightFM(\n",
        "    loss='warp',\n",
        "    no_components=128,\n",
        "    learning_rate=0.01,\n",
        "    item_alpha=1e-06,\n",
        "    user_alpha=0.0001,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "final_model.fit(\n",
        "    train,\n",
        "    user_features=user_features,\n",
        "    item_features=item_features,\n",
        "    epochs=50,\n",
        "    num_threads=4,\n",
        "    verbose=True\n",
        ")\n",
        "final_precision = precision_at_k(\n",
        "    final_model, test,\n",
        "    train_interactions=train,\n",
        "    user_features=user_features,\n",
        "    item_features=item_features,\n",
        "    k=10, num_threads=4\n",
        ").mean()\n",
        "\n",
        "final_recall = recall_at_k(\n",
        "    final_model, test,\n",
        "    train_interactions=train,\n",
        "    user_features=user_features,\n",
        "    item_features=item_features,\n",
        "    k=10, num_threads=4\n",
        ").mean()\n",
        "\n",
        "print(f\"Final precision@10: {final_precision:.4f}, recall@10: {final_recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P3_bObO4jf2",
        "outputId": "02e319d8-2ed7-4ddd-bc26-10eb09ac08fa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final precision@10: 0.1372, recall@10: 0.0750\n"
          ]
        }
      ]
    }
  ]
}